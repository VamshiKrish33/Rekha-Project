{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-output": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (961240782.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install einops --user\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install einops --user\n",
    "pip install einops\n",
    "pip install optuna-integration[pytorch_lightning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.models import convnext_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"image_size\": (128, 128),\n",
    "    \"batch_size\": 16,\n",
    "    \"n_classes\": 3,\n",
    "    \"epochs\": 15,\n",
    "    \"learning_rate\": 0.0001751240503867449,\n",
    "    \"data_dir\": \"/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT\",\n",
    "    \"base_model_save_path\": \"base_cnn_model.pth\",\n",
    "    \"vit_model_save_path\": \"vit_model.pth\",\n",
    "    \"convnext_vit_save_path\": \"final.pth\",\n",
    "    \"vit_patch_size\": 16,\n",
    "    \"vit_dim\": 768,\n",
    "    \"vit_depth\": 6,\n",
    "    \"vit_heads\": 8,\n",
    "    \"vit_mlp_ratio\": 4,\n",
    "    \"vit_dropout\": 0.1,\n",
    "    \"vit_emb_dropout\": 0.1,\n",
    "    \"class_names\": [\"Benign\", \"Malignant\", \"Normal\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class UltrasoundDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        for class_name in [\"benign\", \"malignant\", \"normal\"]:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                if \"mask\" not in file_name.lower():\n",
    "                    image_path = os.path.join(class_dir, file_name)\n",
    "                    mask_path = os.path.join(class_dir, file_name.split(\".\")[0] + \"_mask.png\")\n",
    "                    if os.path.exists(mask_path):\n",
    "                        self.data.append((image_path, mask_path, class_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, mask_path, class_name = self.data[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = {\"benign\": 0, \"malignant\": 1, \"normal\": 2}[class_name]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Data Preparation\n",
    "def prepare_dataloaders(config):\n",
    "    # Augmentations for training set\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(config[\"image_size\"]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    # Minimal transformations for validation set\n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.Resize(config[\"image_size\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    dataset = UltrasoundDataset(config[\"data_dir\"], transform=transform_train)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    sampler = get_sampler(train_ds)\n",
    "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], sampler=sampler)\n",
    "    val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class BaseCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(BaseCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Split image into patches and embed them.\"\"\"\n",
    "    def __init__(self, image_size: int, patch_size: int, in_channels: int = 3, embed_dim: int = 768):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        # Use Conv2d instead of Linear for more efficient patch embedding\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == W == self.image_size, f\"Input image size ({H}*{W}) doesn't match model ({self.image_size}*{self.image_size})\"\n",
    "        \n",
    "        # (B, C, H, W) -> (B, embed_dim, H//patch_size, W//patch_size) -> (B, embed_dim, num_patches)\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head Self Attention mechanism.\"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_dropout(x)\n",
    "        return x\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Transformer MLP block.\"\"\"\n",
    "    def __init__(self, dim: int, mlp_ratio: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mlp_ratio, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadAttention(dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLPBlock(dim, mlp_ratio, dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        attn_output = self.attn(self.norm1(x))\n",
    "        x = x + attn_output  # Out-of-place addition for residual connection\n",
    "        mlp_output = self.mlp(self.norm2(x))\n",
    "        x = x + mlp_output  # Out-of-place addition for residual connection\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int,\n",
    "        patch_size: int,\n",
    "        num_classes: int,\n",
    "        dim: int = 768,\n",
    "        depth: int = 12,\n",
    "        heads: int = 12,\n",
    "        mlp_ratio: int = 4,\n",
    "        dropout: float = 0.1,\n",
    "        emb_dropout: float = 0.1,\n",
    "        channels: int = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "        \n",
    "        self.patch_embed = PatchEmbedding(image_size, patch_size, channels, dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(dim, heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, img: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.patch_embed(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embedding[:, :(n + 1)]  # Out-of-place addition\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = x[:, 0]  # Use CLS token\n",
    "        return self.head(x)\n",
    "\n",
    "    def load_from_mae(self, mae_state_dict: dict) -> None:\n",
    "        \"\"\"Load pre-trained weights from a Masked Autoencoder.\"\"\"\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in mae_state_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torchvision.models import ConvNeXt_Base_Weights\n",
    "from einops import rearrange, repeat\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "\n",
    "class Permute(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "class MeabsAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        \n",
    "        # Initialize weights with smaller values\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        nn.init.xavier_uniform_(self.qkv.weight, gain=0.01)\n",
    "        \n",
    "        # Add input projection for better feature transformation\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.local_enhance = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)  # Add dropout here\n",
    "        )\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Add gradient scaling factor\n",
    "        self.attention_scale = nn.Parameter(torch.ones(1) * 0.1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        x = self.input_proj(x)  # Add input projection\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # Enhanced local features with residual connection\n",
    "        local_features = self.local_enhance(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        q = q + local_features\n",
    "\n",
    "        # Compute attention with scaled dot product\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale * self.attention_scale\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attn = attn.masked_fill(attention_mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return self.proj_dropout(x)\n",
    "\n",
    "\n",
    "class MultiScaleFeatureFusion(nn.Module):\n",
    "    \"\"\"Fuses features from different scales of the ConvNeXt backbone.\"\"\"\n",
    "    def __init__(self, in_channels: List[int], out_channels: int):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_channels, 1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.GELU()\n",
    "            ) for in_ch in in_channels\n",
    "        ])\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * len(in_channels), out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        target_size = features[0].shape[-2:]\n",
    "        processed_features = []\n",
    "        \n",
    "        for feat, conv in zip(features, self.convs):\n",
    "            feat = F.interpolate(feat, size=target_size, mode='bilinear', align_corners=False)\n",
    "            processed_features.append(conv(feat))\n",
    "        \n",
    "        x = torch.cat(processed_features, dim=1)\n",
    "        return self.fusion(x)\n",
    "\n",
    "class MedicalConvNeXtViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        vit_dim: int = 768,\n",
    "        vit_depth: int = 6,\n",
    "        vit_heads: int = 8,\n",
    "        vit_mlp_ratio: int = 4,\n",
    "        dropout: float = 0.3,\n",
    "        emb_dropout: float = 0.2,\n",
    "        aux_loss: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.aux_loss = aux_loss\n",
    "        \n",
    "        # ConvNeXt backbone\n",
    "        self.backbone = convnext_base(weights=ConvNeXt_Base_Weights.IMAGENET1K_V1)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Backbone normalization\n",
    "        self.backbone_norm = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.feature_fusion = MultiScaleFeatureFusion([128, 256, 512, 1024], vit_dim)\n",
    "        \n",
    "        # Calculate the size after feature fusion\n",
    "        self.fused_size = image_size // 32  # ConvNeXt downsamples by factor of 32\n",
    "        \n",
    "        # Patch embedding now properly handles the dimensions\n",
    "        self.patch_embed = nn.Sequential(\n",
    "            nn.Conv2d(vit_dim, vit_dim, kernel_size=patch_size, stride=patch_size),\n",
    "            nn.Flatten(2),  # Flatten spatial dimensions\n",
    "            Permute()\n",
    "        )\n",
    "        \n",
    "        # Calculate number of patches\n",
    "        self.num_patches = (self.fused_size // patch_size) ** 2\n",
    "        \n",
    "        # Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, vit_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, self.num_patches + 1, vit_dim))\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
    "        \n",
    "        self.pos_drop = nn.Dropout(emb_dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(vit_dim, vit_heads, vit_mlp_ratio, dropout)\n",
    "            for _ in range(vit_depth)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(vit_dim)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(vit_dim, vit_dim),\n",
    "            nn.LayerNorm(vit_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(vit_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        if aux_loss:\n",
    "            self.aux_head = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(1024, num_classes)\n",
    "            )\n",
    "\n",
    "    def extract_features(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
    "        features = []\n",
    "        for i, layer in enumerate(self.backbone.features):\n",
    "            x = layer(x)\n",
    "            if i in [1, 3, 5, 7]:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Get backbone features\n",
    "        if self.training:\n",
    "            conv_features = []\n",
    "            for i, layer in enumerate(self.backbone.features):\n",
    "                x = layer(x)\n",
    "                if i in [1, 3, 5, 7]:\n",
    "                    conv_features.append(x)\n",
    "        else:\n",
    "            conv_features = self.extract_features(x)\n",
    "        \n",
    "        # Normalize last features\n",
    "        conv_features[-1] = self.backbone_norm(conv_features[-1])\n",
    "        \n",
    "        # Fuse features\n",
    "        x = self.feature_fusion(conv_features)  # Shape: [B, vit_dim, H, W]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # Shape: [B, num_patches, vit_dim]\n",
    "        \n",
    "        # Add cls token\n",
    "        cls_token = repeat(self.cls_token, '1 1 d -> b 1 d', b=x.shape[0])\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        \n",
    "        # Add position embedding\n",
    "        if x.size(1) != self.pos_embedding.size(1):\n",
    "            # Interpolate position embeddings if needed\n",
    "            pos_embedding = F.interpolate(\n",
    "                self.pos_embedding.transpose(1, 2),\n",
    "                size=x.size(1),\n",
    "                mode='linear'\n",
    "            ).transpose(1, 2)\n",
    "        else:\n",
    "            pos_embedding = self.pos_embedding\n",
    "            \n",
    "        x = x + pos_embedding\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Transform features\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]\n",
    "        main_out = self.head(cls_token_final)\n",
    "        \n",
    "        if self.aux_loss and self.training:\n",
    "            aux_out = self.aux_head(conv_features[-1])\n",
    "            return main_out, aux_out\n",
    "            \n",
    "        return main_out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Focal Loss implementation.\n",
    "        Args:\n",
    "            gamma (float): Focusing parameter for modulating factor (1-p).\n",
    "            alpha (Tensor, optional): Class weights. If provided, should have the same length as the number of classes.\n",
    "            reduction (str): Specifies the reduction to apply to the output. Options are 'none', 'mean', or 'sum'.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass for Focal Loss.\n",
    "        Args:\n",
    "            inputs (Tensor): Predictions from the model (logits).\n",
    "            targets (Tensor): Ground truth labels.\n",
    "        Returns:\n",
    "            Tensor: Computed Focal Loss.\n",
    "        \"\"\"\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')  # Compute Cross-Entropy loss per sample\n",
    "        pt = torch.exp(-ce_loss)  # Probabilities of the true class\n",
    "\n",
    "        # Compute Focal Loss\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        # Apply class weights if provided\n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha[targets].to(inputs.device)  # Select class-specific weights for each sample\n",
    "            focal_loss = alpha * focal_loss\n",
    "\n",
    "        # Reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeightedRandomSampler\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cosine_schedule_with_warmup\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_score, recall_score, f1_score\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# Add these imports\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from torch.amp import GradScaler\n",
    "from torch.amp import autocast\n",
    "\n",
    "def get_sampler(dataset):\n",
    "    \"\"\"Create a weighted sampler to handle class imbalance\"\"\"\n",
    "    labels = [label for _, label in dataset]\n",
    "    class_counts = np.bincount(labels)\n",
    "    weights = 1.0 / class_counts\n",
    "    sample_weights = weights[labels]\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "    return sampler\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, config, device, class_weights=None, save_dir=\"./results\"):\n",
    "    \"\"\"Enhanced training function with stability improvements, best F1 tracking, and confusion matrix saving.\"\"\"\n",
    "    \n",
    "    # Loss function with class weights\n",
    "    criterion = FocalLoss(gamma=3, alpha=class_weights)\n",
    "    \n",
    "    # Optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=1.5,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Cosine learning rate scheduler with warmup\n",
    "    num_training_steps = config['epochs'] * len(train_loader)\n",
    "    num_warmup_steps = num_training_steps // 10\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    # Gradient scaler for mixed precision training\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Track best F1 score\n",
    "    best_f1 = 0\n",
    "    best_model_weights = None\n",
    "    best_epoch_preds, best_epoch_labels = None, None\n",
    "\n",
    "    # Initialize metrics storage\n",
    "    metrics = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"val_f1\": [],\n",
    "        \"train_precision\": [],\n",
    "        \"val_precision\": [],\n",
    "        \"train_recall\": [],\n",
    "        \"val_recall\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Gradient scaling and backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Optimizer step with gradient scaling\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Compute metrics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "        \n",
    "        # Calculate detailed training metrics\n",
    "        train_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        train_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_preds, val_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                with autocast(device_type=\"cuda\"):\n",
    "                    outputs = model(inputs)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                \n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(targets.cpu().numpy())\n",
    "        \n",
    "        # Calculate detailed validation metrics\n",
    "        val_precision = precision_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "        val_recall = recall_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "\n",
    "        # Track the best F1 score and save the model weights\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_weights = model.state_dict()\n",
    "            best_epoch_preds = val_preds\n",
    "            best_epoch_labels = val_labels\n",
    "\n",
    "        # Compute and print epoch metrics\n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        # Append metrics for this epoch\n",
    "        metrics[\"train_loss\"].append(train_loss / len(train_loader))\n",
    "        metrics[\"val_loss\"].append(val_loss / len(val_loader))\n",
    "        metrics[\"train_acc\"].append(train_acc)\n",
    "        metrics[\"val_acc\"].append(val_acc)\n",
    "        metrics[\"train_f1\"].append(train_f1)\n",
    "        metrics[\"val_f1\"].append(val_f1)\n",
    "        metrics[\"train_precision\"].append(train_precision)\n",
    "        metrics[\"val_precision\"].append(val_precision)\n",
    "        metrics[\"train_recall\"].append(train_recall)\n",
    "        metrics[\"val_recall\"].append(val_recall)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1}/{config[\"epochs\"]}')\n",
    "        print(f'Train Loss: {metrics[\"train_loss\"][-1]:.4f}, '\n",
    "              f'Train Acc: {metrics[\"train_acc\"][-1]:.4f}, '\n",
    "              f'Train Precision: {metrics[\"train_precision\"][-1]:.4f}, '\n",
    "              f'Train Recall: {metrics[\"train_recall\"][-1]:.4f}, '\n",
    "              f'Train F1: {metrics[\"train_f1\"][-1]:.4f}')\n",
    "        print(f'Val Loss: {metrics[\"val_loss\"][-1]:.4f}, '\n",
    "              f'Val Acc: {metrics[\"val_acc\"][-1]:.4f}, '\n",
    "              f'Val Precision: {metrics[\"val_precision\"][-1]:.4f}, '\n",
    "              f'Val Recall: {metrics[\"val_recall\"][-1]:.4f}, '\n",
    "              f'Val F1: {metrics[\"val_f1\"][-1]:.4f}')\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Save confusion matrix and print misclassified images for the best epoch\n",
    "    print(f\"Best F1 Score: {best_f1:.4f}\")\n",
    "    save_confusion_matrix(best_epoch_labels, best_epoch_preds, model_name=config['model_name'], save_dir=save_dir)\n",
    "    print_misclassified_images(best_epoch_labels, best_epoch_preds, val_loader.dataset, num_examples=5)\n",
    "\n",
    "    # Return metrics and best weights\n",
    "    return metrics, best_model_weights\n",
    "\n",
    "\n",
    "def save_confusion_matrix(labels, preds, model_name, save_dir):\n",
    "    \"\"\"Save confusion matrix as an image.\"\"\"\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"{model_name}_best_confusion_matrix.png\")\n",
    "    disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
    "    disp.figure_.savefig(save_path)\n",
    "    print(f\"Confusion matrix saved to {save_path}\")\n",
    "\n",
    "\n",
    "def print_misclassified_images(labels, preds, dataset, num_examples=5):\n",
    "    \"\"\"Print and visualize some misclassified images.\"\"\"\n",
    "    misclassified_indices = [i for i, (y_true, y_pred) in enumerate(zip(labels, preds)) if y_true != y_pred]\n",
    "    print(f\"Number of misclassified examples: {len(misclassified_indices)}\")\n",
    "    \n",
    "    if len(misclassified_indices) == 0:\n",
    "        print(\"No misclassified examples found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Displaying up to {num_examples} misclassified images:\")\n",
    "    for idx in misclassified_indices[:num_examples]:\n",
    "        image, true_label = dataset[idx]\n",
    "        predicted_label = preds[idx]\n",
    "\n",
    "        # Display the image and labels\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.permute(1, 2, 0).cpu().numpy()  # Convert CHW to HWC for display\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"True: {true_label}, Predicted: {predicted_label}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "def print_metrics(labels, preds):\n",
    "    \"\"\"Print detailed classification metrics\"\"\"\n",
    "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, config, device):\n",
    "    if model_name == \"base_cnn\":\n",
    "        model = BaseCNN(config[\"n_classes\"]).to(device)\n",
    "        save_path = config[\"base_model_save_path\"]\n",
    "    elif model_name == \"vit\":\n",
    "        model = ViT(\n",
    "            image_size=config[\"image_size\"][0],\n",
    "            patch_size=config[\"vit_patch_size\"],\n",
    "            num_classes=config[\"n_classes\"],\n",
    "            dim=config[\"vit_dim\"],\n",
    "            depth=config[\"vit_depth\"],\n",
    "            heads=config[\"vit_heads\"],\n",
    "            mlp_ratio=config[\"vit_mlp_ratio\"],\n",
    "            dropout=config[\"vit_dropout\"],\n",
    "            emb_dropout=config[\"vit_emb_dropout\"]\n",
    "        ).to(device)\n",
    "        save_path = config[\"vit_model_save_path\"]\n",
    "    elif model_name == \"convnext_vit\":\n",
    "        model = MedicalConvNeXtViT(\n",
    "            num_classes=config[\"n_classes\"],\n",
    "            image_size=config[\"image_size\"][0],\n",
    "            patch_size=config[\"vit_patch_size\"],\n",
    "            vit_dim=config[\"vit_dim\"],\n",
    "            vit_depth=config[\"vit_depth\"],\n",
    "            vit_heads=config[\"vit_heads\"],\n",
    "            vit_mlp_ratio=config[\"vit_mlp_ratio\"],\n",
    "            dropout=config[\"vit_dropout\"],\n",
    "            emb_dropout=config[\"vit_emb_dropout\"]\n",
    "        ).to(device)\n",
    "        save_path = config[\"convnext_vit_save_path\"]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "\n",
    "    return model, save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_dict, metric_names=[\"Loss\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]):\n",
    "    \"\"\"\n",
    "    Plots training and validation metrics for multiple models.\n",
    "    \n",
    "    Args:\n",
    "        metrics_dict (dict): Dictionary containing metric lists for multiple models. \n",
    "                             Format: \n",
    "                             {\n",
    "                                 \"model_name\": {\n",
    "                                     \"train_loss\": [...],\n",
    "                                     \"val_loss\": [...],\n",
    "                                     \"train_acc\": [...],\n",
    "                                     \"val_acc\": [...],\n",
    "                                     \"train_precision\": [...],\n",
    "                                     \"val_precision\": [...],\n",
    "                                     \"train_recall\": [...],\n",
    "                                     \"val_recall\": [...],\n",
    "                                     \"train_f1\": [...],\n",
    "                                     \"val_f1\": [...]\n",
    "                                 }\n",
    "                             }\n",
    "        metric_names (list): List of metric names to plot.\n",
    "    \"\"\"\n",
    "    num_epochs = len(next(iter(metrics_dict.values()))[\"train_loss\"])\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    \n",
    "    for metric_name in metric_names:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for idx, (model_name, metrics) in enumerate(metrics_dict.items()):\n",
    "            train_metric = metrics.get(f\"train_{metric_name.lower()}\", [])\n",
    "            val_metric = metrics.get(f\"val_{metric_name.lower()}\", [])\n",
    "            \n",
    "            if train_metric:\n",
    "                plt.plot(epochs, train_metric, linestyle='--', color=colors[idx % len(colors)], \n",
    "                         label=f\"{model_name} Train {metric_name}\")\n",
    "            if val_metric:\n",
    "                plt.plot(epochs, val_metric, color=colors[idx % len(colors)], \n",
    "                         label=f\"{model_name} Val {metric_name}\")\n",
    "        \n",
    "        plt.title(f\"{metric_name} Comparison Across Models\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(metric_name)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Prepare DataLoaders\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_loader, val_loader, train_ds, val_ds \u001b[38;5;241m=\u001b[39m prepare_dataloaders(config)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize metrics dictionary\u001b[39;00m\n\u001b[1;32m      9\u001b[0m metrics_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m, in \u001b[0;36mprepare_dataloaders\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Minimal transformations for validation set\u001b[39;00m\n\u001b[1;32m     39\u001b[0m transform_val \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     40\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     41\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     42\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m])\n\u001b[1;32m     43\u001b[0m ])\n\u001b[0;32m---> 45\u001b[0m dataset \u001b[38;5;241m=\u001b[39m UltrasoundDataset(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], transform\u001b[38;5;241m=\u001b[39mtransform_train)\n\u001b[1;32m     46\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[1;32m     47\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m train_size\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mUltrasoundDataset.__init__\u001b[0;34m(self, root_dir, transform)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenign\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmalignant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      8\u001b[0m     class_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, class_name)\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(class_dir):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mlower():\n\u001b[1;32m     11\u001b[0m             image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_dir, file_name)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Device Configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Prepare DataLoaders\n",
    "    train_loader, val_loader, train_ds, val_ds = prepare_dataloaders(config)\n",
    "\n",
    "    # Initialize metrics dictionary\n",
    "    metrics_dict = {}\n",
    "\n",
    "    # Compute class weights for weighted loss\n",
    "    class_labels = [label for _, label in train_ds]\n",
    "    class_weights = compute_class_weight(\"balanced\", classes=range(config[\"n_classes\"]), y=class_labels)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    for model_name in [\"base_cnn\", \"vit\", \"convnext_vit\"]:\n",
    "        print(f\"\\n{'='*20} Training {model_name.upper()} {'='*20}\")\n",
    "    \n",
    "        # Initialize model\n",
    "        model, save_path = initialize_model(model_name, config, device)\n",
    "    \n",
    "        # Add model name to config\n",
    "        config[\"model_name\"] = model_name\n",
    "    \n",
    "        # Training configuration\n",
    "        weights = class_weights if model_name == \"convnext_vit\" else None\n",
    "    \n",
    "        # Train and evaluate\n",
    "        metrics, best_weights = train_and_evaluate(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            config=config,\n",
    "            device=device,\n",
    "            class_weights=weights,\n",
    "            save_dir=f\"./results/{model_name}\"\n",
    "        )\n",
    "    \n",
    "        # Save the best model weights\n",
    "        if best_weights:\n",
    "            torch.save(best_weights, save_path)\n",
    "            print(f\"Best model for {model_name.upper()} saved at {save_path}\")\n",
    "    \n",
    "        # Store metrics for comparison\n",
    "        metrics_dict[model_name.upper()] = metrics\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim import AdamW\n",
    "# from transformers import get_cosine_schedule_with_warmup\n",
    "# from torch.amp import GradScaler, autocast\n",
    "# from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "# import optuna\n",
    "# import numpy as np\n",
    "# from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "# def get_sampler(dataset):\n",
    "#     \"\"\"Create a weighted sampler to handle class imbalance.\"\"\"\n",
    "#     labels = [label for _, label in dataset]\n",
    "#     class_counts = np.bincount(labels)\n",
    "#     weights = 1.0 / class_counts\n",
    "#     sample_weights = weights[labels]\n",
    "#     sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "#     return sampler\n",
    "\n",
    "# def train_step(model, train_loader, optimizer, criterion, scaler, scheduler, device):\n",
    "#     \"\"\"Performs a single training epoch.\"\"\"\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     for inputs, targets in train_loader:\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         with autocast(device_type=\"cuda\"):\n",
    "#             outputs = model(inputs)\n",
    "#             if isinstance(outputs, tuple):  # Handle auxiliary outputs\n",
    "#                 outputs = outputs[0]\n",
    "#             loss = criterion(outputs, targets)\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         # Compute metrics\n",
    "#         total_loss += loss.item()\n",
    "#         _, predicted = outputs.max(1)\n",
    "#         total += targets.size(0)\n",
    "#         correct += predicted.eq(targets).sum().item()\n",
    "#         all_preds.extend(predicted.cpu().numpy())\n",
    "#         all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "#     accuracy = correct / total\n",
    "#     return total_loss / len(train_loader), accuracy, all_preds, all_labels\n",
    "\n",
    "\n",
    "# def validate_step(model, val_loader, criterion, device):\n",
    "#     \"\"\"Performs validation and computes metrics.\"\"\"\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in val_loader:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "#             with autocast(device_type=\"cuda\"):\n",
    "#                 outputs = model(inputs)\n",
    "#                 if isinstance(outputs, tuple):  # Handle auxiliary outputs\n",
    "#                     outputs = outputs[0]\n",
    "#                 loss = criterion(outputs, targets)\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "#             _, predicted = outputs.max(1)\n",
    "#             total += targets.size(0)\n",
    "#             correct += predicted.eq(targets).sum().item()\n",
    "#             all_preds.extend(predicted.cpu().numpy())\n",
    "#             all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "#     accuracy = correct / total\n",
    "#     f1 = f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "#     precision = precision_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "#     recall = recall_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "\n",
    "#     return total_loss / len(val_loader), accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# def objective(trial, train_loader, val_loader, config, device):\n",
    "#     \"\"\"Objective function for Optuna hyperparameter optimization.\"\"\"\n",
    "#     # Suggest hyperparameters\n",
    "#     learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "#     weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
    "#     gamma = trial.suggest_float(\"gamma\", 1.0, 5.0, step=0.5)\n",
    "\n",
    "#     # Initialize model\n",
    "#     model = MedicalConvNeXtViT(\n",
    "#         num_classes=config[\"n_classes\"],\n",
    "#         image_size=config[\"image_size\"][0],\n",
    "#         patch_size=config[\"vit_patch_size\"],\n",
    "#         vit_dim=config[\"vit_dim\"],\n",
    "#         vit_depth=config[\"vit_depth\"],\n",
    "#         vit_heads=config[\"vit_heads\"],\n",
    "#         vit_mlp_ratio=config[\"vit_mlp_ratio\"],\n",
    "#         dropout=config[\"vit_dropout\"],\n",
    "#         emb_dropout=config[\"vit_emb_dropout\"],\n",
    "#         aux_loss=True\n",
    "#     ).to(device)\n",
    "\n",
    "#     # Compute class weights for the dataset\n",
    "#     class_weights = compute_class_weight(\n",
    "#         \"balanced\", classes=range(config[\"n_classes\"]), y=[label for _, label in train_loader.dataset]\n",
    "#     )\n",
    "#     class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "#     # Define loss function, optimizer, and scheduler\n",
    "#     criterion = FocalLoss(gamma=trial.suggest_float(\"gamma\", 1.0, 5.0, step=0.5), alpha=class_weights)\n",
    "#     optimizer = AdamW(\n",
    "#         model.parameters(),\n",
    "#         lr=learning_rate,\n",
    "#         weight_decay=weight_decay,\n",
    "#         betas=(0.9, 0.999)\n",
    "#     )\n",
    "#     num_training_steps = config[\"epochs\"] * len(train_loader)\n",
    "#     scheduler = get_cosine_schedule_with_warmup(\n",
    "#         optimizer,\n",
    "#         num_warmup_steps=num_training_steps // 10,\n",
    "#         num_training_steps=num_training_steps\n",
    "#     )\n",
    "#     scaler = GradScaler()\n",
    "\n",
    "#     # Training loop\n",
    "#     best_f1 = 0\n",
    "#     for epoch in range(config[\"epochs\"]):\n",
    "#         train_loss, train_acc, _, _ = train_step(model, train_loader, optimizer, criterion, scaler, scheduler, device)\n",
    "#         val_loss, val_acc, precision, recall, f1 = validate_step(model, val_loader, criterion, device)\n",
    "\n",
    "#         # Report metrics to Optuna\n",
    "#         trial.report(f1, epoch)\n",
    "\n",
    "#         # Early stopping\n",
    "#         if trial.should_prune():\n",
    "#             raise optuna.TrialPruned()\n",
    "\n",
    "#         # Save best F1-score\n",
    "#         best_f1 = max(best_f1, f1)\n",
    "\n",
    "#         print(f\"Epoch {epoch + 1}/{config['epochs']}:\")\n",
    "#         print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "#         print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "#     return best_f1\n",
    "\n",
    "\n",
    "# # Main function to run Optuna\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Device configuration\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     # Prepare DataLoaders\n",
    "#     train_loader, val_loader, train_ds, val_ds = prepare_dataloaders(config)\n",
    "\n",
    "#     # Create Optuna study\n",
    "#     study = optuna.create_study(direction=\"maximize\")\n",
    "#     study.optimize(lambda trial: objective(trial, train_loader, val_loader, config, device), n_trials=20)\n",
    "\n",
    "#     # Print best trial\n",
    "#     print(\"\\nBest Trial:\")\n",
    "#     print(study.best_trial.params)\n",
    "#     print(f\"Best Validation F1-score: {study.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1209633,
     "sourceId": 2021025,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
